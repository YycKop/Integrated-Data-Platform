# Copyright (c) 2025 YycKop
# MIT License
# Integrated-Data-Platform-backend/datasets/views.py
from rest_framework import viewsets, permissions, status
from rest_framework.response import Response
from rest_framework.decorators import action
from rest_framework.parsers import MultiPartParser, FormParser
from django.utils import timezone
from rest_framework.pagination import PageNumberPagination
from django.db.models import Q
import json
import pandas as pd
from io import StringIO, BytesIO

# ç¡®ä¿æ­£ç¡®å¯¼å…¥ DatabaseConnector
from .models import DataSource, Dataset, DataRecord
from .serializers import DataSourceSerializer, DatasetSerializer, DataRecordSerializer
from .db_utils import DatabaseConnector

# å¯¼å…¥æ´»åŠ¨è®°å½•åŠŸèƒ½
from activities.utils import create_dataset_activity, create_data_source_activity


class IsAdminOrReadOnly(permissions.BasePermission):
    """ç®¡ç†å‘˜å¯ä»¥ç¼–è¾‘ï¼Œå…¶ä»–è®¤è¯ç”¨æˆ·åªèƒ½æŸ¥çœ‹"""

    def has_permission(self, request, view):
        if request.method in permissions.SAFE_METHODS:
            return request.user.is_authenticated
        return request.user.is_authenticated and request.user.role == 'admin'


class DataSourceViewSet(viewsets.ModelViewSet):
    queryset = DataSource.objects.all()
    serializer_class = DataSourceSerializer
    permission_classes = [permissions.IsAuthenticated, IsAdminOrReadOnly]

    def get_queryset(self):
        # æ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ•°æ®æº
        return DataSource.objects.all()

    def get_permissions(self):
        if self.action in ['list', 'retrieve', 'files']:
            # å¯¹äºè¯»å–æ“ä½œï¼Œæ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½æœ‰æƒé™
            return [permissions.IsAuthenticated()]
        # å¯¹äºä¿®æ”¹æ“ä½œï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™
        return [permissions.IsAuthenticated(), IsAdminOrReadOnly()]

    def perform_create(self, serializer):
        try:
            data_source = serializer.save(created_by=self.request.user)
            print(f"ğŸ”§ [DataSource] æ­£åœ¨åˆ›å»ºæ•°æ®æº: {data_source.name}, ID: {data_source.id}")
            print(f"ğŸ”§ [DataSource] å½“å‰ç”¨æˆ·: {self.request.user.username}")

            # åˆ›å»ºæ´»åŠ¨è®°å½• - ä¿®å¤è°ƒç”¨
            from activities.utils import create_data_source_activity
            result = create_data_source_activity(
                user=self.request.user,
                data_source_name=data_source.name,
                data_source_id=data_source.id,
                action='created'
            )

            if result:
                print(f"âœ… [DataSource] æˆåŠŸåˆ›å»ºæ•°æ®æºæ´»åŠ¨è®°å½•: {result.description}")
            else:
                print(f"âŒ [DataSource] åˆ›å»ºæ•°æ®æºæ´»åŠ¨è®°å½•å¤±è´¥")

        except Exception as e:
            print(f"âŒ [DataSource] åˆ›å»ºæ•°æ®æºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    def perform_destroy(self, instance):
        print(f"ğŸ”§ [DataSource] æ­£åœ¨åˆ é™¤æ•°æ®æº: {instance.name}, ID: {instance.id}")

        # åˆ›å»ºåˆ é™¤æ´»åŠ¨è®°å½•
        create_data_source_activity(
            user=self.request.user,
            data_source_name=instance.name,
            data_source_id=instance.id,
            action='deleted'
        )

        print(f"âœ… [DataSource] æ•°æ®æºåˆ é™¤æ´»åŠ¨è®°å½•å·²åˆ›å»º")
        super().perform_destroy(instance)

    @action(detail=True, methods=['post'], parser_classes=[MultiPartParser, FormParser])
    def upload_file(self, request, pk=None):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æ•°æ®æº"""
        data_source = self.get_object()
        file = request.FILES.get('file')

        if not file:
            return Response({'error': 'æ²¡æœ‰æä¾›æ–‡ä»¶'}, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥ç”¨æˆ·æƒé™ - åªæœ‰åˆ›å»ºè€…æˆ–ç®¡ç†å‘˜å¯ä»¥ä¸Šä¼ æ–‡ä»¶
        if data_source.created_by != request.user and request.user.role != 'admin':
            return Response({'error': 'æ²¡æœ‰æƒé™è®¿é—®æ­¤æ•°æ®æº'}, status=status.HTTP_403_FORBIDDEN)

        try:
            print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.name}, å¤§å°: {file.size} bytes")

            file_info = {
                'filename': file.name,
                'size': file.size,
                'content_type': file.content_type,
                'upload_time': timezone.now().isoformat()
            }

            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
            if file.name.lower().endswith('.json'):
                return self._handle_json_file(file, data_source, request.user, file_info)
            elif file.name.lower().endswith('.csv'):
                return self._handle_csv_file(file, data_source, request.user, file_info)
            elif file.name.lower().endswith(('.xlsx', '.xls')):
                return self._handle_excel_file(file, data_source, request.user, file_info)
            else:
                return Response({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ CSVã€Excelã€JSON'},
                                status=status.HTTP_400_BAD_REQUEST)

        except Exception as e:
            print(f"æ–‡ä»¶å¤„ç†å¼‚å¸¸: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return Response({'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'},
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _handle_json_file(self, file, data_source, user, file_info):
        """å¤„ç†JSONæ–‡ä»¶ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            file.seek(0)

            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read().decode('utf-8')

            if not file_content.strip():
                return Response({'error': 'JSONæ–‡ä»¶ä¸ºç©º'}, status=status.HTTP_400_BAD_REQUEST)

            # è§£æJSON
            json_data = json.loads(file_content)
            print(f"åŸå§‹JSONæ•°æ®ç±»å‹: {type(json_data)}")

            # ç»Ÿä¸€å¤„ç†ä¸ºè®°å½•åˆ—è¡¨
            records_to_save = []

            if isinstance(json_data, list):
                # å¦‚æœæ˜¯æ•°ç»„æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                records_to_save = json_data
                print(f"æ•°ç»„æ ¼å¼ï¼Œè®°å½•æ•°: {len(records_to_save)}")

            elif isinstance(json_data, dict):
                # å¦‚æœæ˜¯å¯¹è±¡æ ¼å¼ï¼ŒæŸ¥æ‰¾å¸¸è§çš„æ•°æ®å­—æ®µ
                data_fields = ['RECORDS', 'records', 'data', 'items', 'results']
                found_data = False

                for field in data_fields:
                    if field in json_data and isinstance(json_data[field], list):
                        records_to_save = json_data[field]
                        print(f"æ‰¾åˆ°æ•°æ®å­—æ®µ '{field}'ï¼Œè®°å½•æ•°: {len(records_to_save)}")
                        found_data = True
                        break

                if not found_data:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®å­—æ®µï¼Œå°†æ•´ä¸ªå¯¹è±¡ä½œä¸ºå•æ¡è®°å½•
                    records_to_save = [json_data]
                    print("å¯¹è±¡æ ¼å¼ï¼Œä½œä¸ºå•æ¡è®°å½•å¤„ç†")
            else:
                return Response({'error': 'ä¸æ”¯æŒçš„JSONæ ¼å¼'}, status=status.HTTP_400_BAD_REQUEST)

            if not records_to_save:
                return Response({'error': 'JSONæ•°æ®ä¸ºç©º'}, status=status.HTTP_400_BAD_REQUEST)

            # æ¸…ç†æ•°æ®ï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
            cleaned_records = []
            for record in records_to_save:
                try:
                    cleaned_record = self._clean_json_data(record)
                    cleaned_records.append(cleaned_record)
                except Exception as e:
                    print(f"æ¸…ç†è®°å½•æ—¶å‡ºé”™ï¼Œè·³è¿‡: {str(e)}")
                    continue

            if not cleaned_records:
                return Response({'error': 'æ‰€æœ‰æ•°æ®è®°å½•éƒ½æ— æ³•å¤„ç†'}, status=status.HTTP_400_BAD_REQUEST)

            # åˆ›å»ºæ•°æ®é›†
            dataset_name = f"{data_source.name} - {file.name}"
            dataset = Dataset.objects.create(
                name=dataset_name,
                data_source=data_source,
                data_type='json',
                description=f'ä»æ–‡ä»¶ {file.name} å¯¼å…¥çš„JSONæ•°æ®ï¼Œå¤§å°: {file.size} å­—èŠ‚',
                data_structure={'fields': list(cleaned_records[0].keys())} if cleaned_records else {},
                created_by=user
            )

            # æ‰¹é‡åˆ›å»ºæ•°æ®è®°å½•
            records_created = 0
            batch_size = 50

            for i in range(0, len(cleaned_records), batch_size):
                batch = cleaned_records[i:i + batch_size]
                data_records = [
                    DataRecord(dataset=dataset, data=record)
                    for record in batch
                ]
                DataRecord.objects.bulk_create(data_records)
                records_created += len(batch)

            # åˆ›å»ºæ•°æ®é›†æ´»åŠ¨è®°å½•
            create_dataset_activity(
                user=user,
                dataset_name=dataset.name,
                dataset_id=dataset.id,
                action='created'
            )

            file_info['records_created'] = records_created
            file_info['dataset_id'] = dataset.id
            file_info['total_records'] = len(cleaned_records)

            return Response({
                'message': 'JSONæ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
                'file_info': file_info,
                'data_source_id': data_source.id,
                'records_created': records_created,
                'total_records': len(cleaned_records),
                'dataset_id': dataset.id
            })

        except json.JSONDecodeError as e:
            return Response({'error': f'JSONæ–‡ä»¶è§£æå¤±è´¥: {str(e)}'}, status=status.HTTP_400_BAD_REQUEST)
        except Exception as e:
            print(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return Response({'error': f'å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _handle_csv_file(self, file, data_source, user, file_info):
        """å¤„ç†CSVæ–‡ä»¶ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            file.seek(0)

            # ä½¿ç”¨pandasè¯»å–CSVæ–‡ä»¶ï¼Œå¤„ç†å„ç§ç¼–ç 
            file_content = file.read()

            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            df = None

            for encoding in encodings:
                try:
                    file_content_decoded = file_content.decode(encoding)
                    df = pd.read_csv(StringIO(file_content_decoded))
                    print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶")
                    break
                except (UnicodeDecodeError, Exception) as e:
                    print(f"ç¼–ç  {encoding} å¤±è´¥: {str(e)}")
                    continue

            if df is None:
                return Response({'error': 'æ— æ³•è§£æCSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç '},
                                status=status.HTTP_400_BAD_REQUEST)

            return self._handle_dataframe(df, file, data_source, user, file_info, 'csv')

        except Exception as e:
            print(f"å¤„ç†CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return Response({'error': f'å¤„ç†CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'},
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _handle_excel_file(self, file, data_source, user, file_info):
        """å¤„ç†Excelæ–‡ä»¶"""
        try:
            # ä½¿ç”¨pandasè¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(BytesIO(file.read()))

            return self._handle_dataframe(df, file, data_source, user, file_info, 'excel')

        except Exception as e:
            return Response({'error': f'å¤„ç†Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _handle_dataframe(self, df, file, data_source, user, file_info, data_type):
        """é€šç”¨DataFrameå¤„ç†é€»è¾‘ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            if df.empty:
                return Response({'error': 'æ–‡ä»¶æ•°æ®ä¸ºç©º'}, status=status.HTTP_400_BAD_REQUEST)

            # æ¸…ç†æ•°æ®ï¼šå¤„ç†NaNå€¼å’ŒéJSONå¯åºåˆ—åŒ–ç±»å‹
            df_cleaned = df.where(pd.notnull(df), None)

            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            for col in df_cleaned.columns:
                # è½¬æ¢éJSONå¯åºåˆ—åŒ–çš„ç±»å‹
                if df_cleaned[col].dtype == 'object':
                    df_cleaned[col] = df_cleaned[col].apply(
                        lambda x: str(x) if not isinstance(x, (str, int, float, bool, type(None))) else x
                    )

            # åˆ›å»ºæ•°æ®é›†
            dataset_name = f"{data_source.name} - {file.name}"
            dataset = Dataset.objects.create(
                name=dataset_name,
                data_source=data_source,
                data_type=data_type,
                description=f'ä»æ–‡ä»¶ {file.name} å¯¼å…¥çš„æ•°æ®ï¼Œå¤§å°: {file.size} å­—èŠ‚',
                data_structure={'fields': df_cleaned.columns.tolist()},
                created_by=user
            )

            print(f"ğŸ”§ [Dataset] æ­£åœ¨åˆ›å»ºæ•°æ®é›†: {dataset.name}, ID: {dataset.id}")

            # åˆ›å»ºæ´»åŠ¨è®°å½•
            result = create_dataset_activity(
                user=user,
                dataset_name=dataset.name,
                dataset_id=dataset.id,
                action='created'
            )

            if result:
                print(f"âœ… [Dataset] æˆåŠŸåˆ›å»ºæ•°æ®é›†æ´»åŠ¨è®°å½•")
            else:
                print(f"âŒ [Dataset] åˆ›å»ºæ•°æ®é›†æ´»åŠ¨è®°å½•å¤±è´¥")

            # æ‰¹é‡åˆ›å»ºæ•°æ®è®°å½• - ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°
            records_created = 0
            batch_size = 50  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…JSONéªŒè¯é—®é¢˜

            for i in range(0, len(df_cleaned), batch_size):
                batch_df = df_cleaned.iloc[i:i + batch_size]
                data_records = []

                for _, row in batch_df.iterrows():
                    try:
                        # ç¡®ä¿æ•°æ®æ˜¯æœ‰æ•ˆçš„JSON
                        row_data = row.to_dict()

                        # è¿›ä¸€æ­¥æ¸…ç†æ•°æ®
                        cleaned_data = {}
                        for key, value in row_data.items():
                            if pd.isna(value) or value is None:
                                cleaned_data[key] = None
                            elif isinstance(value, (pd.Timestamp, pd.DatetimeIndex)):
                                cleaned_data[key] = value.isoformat()
                            elif isinstance(value, (int, float, str, bool)):
                                cleaned_data[key] = value
                            else:
                                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                cleaned_data[key] = str(value)

                        # éªŒè¯JSONå¯åºåˆ—åŒ–
                        import json
                        json.dumps(cleaned_data)  # å¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸

                        data_records.append(DataRecord(
                            dataset=dataset,
                            data=cleaned_data
                        ))

                    except Exception as e:
                        print(f"å¤„ç†è¡Œæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                        # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œï¼Œç»§ç»­å¤„ç†å…¶ä»–è¡Œ
                        continue

                if data_records:
                    DataRecord.objects.bulk_create(data_records)
                    records_created += len(data_records)

            # åˆ›å»ºæ•°æ®é›†æ´»åŠ¨è®°å½•
            create_dataset_activity(
                user=user,
                dataset_name=dataset.name,
                dataset_id=dataset.id,
                action='created'
            )

            file_info['records_created'] = records_created
            file_info['dataset_id'] = dataset.id
            file_info['total_rows'] = len(df_cleaned)

            return Response({
                'message': f'{data_type.upper()}æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
                'file_info': file_info,
                'data_source_id': data_source.id,
                'records_created': records_created,
                'total_rows': len(df_cleaned)
            })

        except Exception as e:
            print(f"å¤„ç†DataFrameæ—¶å‡ºé”™: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return Response({'error': f'å¤„ç†æ•°æ®æ—¶å‡ºé”™: {str(e)}'},
                            status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    @action(detail=True, methods=['post'])
    def import_table(self, request, pk=None):
        """å¯¼å…¥è¡¨æ•°æ®"""
        data_source = self.get_object()
        table_name = request.data.get('table_name')
        dataset_name = request.data.get('dataset_name', f'{data_source.name} - {table_name}')

        print(f"å¼€å§‹å¯¼å…¥è¡¨: {table_name}")
        print(f"æ•°æ®æº: {data_source.id} - {data_source.name}")
        print(f"æ•°æ®é›†åç§°: {dataset_name}")
        print(f"è¿æ¥é…ç½®: {data_source.connection_config}")

        if not table_name:
            return Response({'error': 'ç¼ºå°‘è¡¨å'}, status=status.HTTP_400_BAD_REQUEST)

        if data_source.type != 'database':
            return Response({'error': 'æ­¤æ•°æ®æºä¸æ˜¯æ•°æ®åº“ç±»å‹'}, status=status.HTTP_400_BAD_REQUEST)

        # æ£€æŸ¥ç”¨æˆ·æƒé™ - åªæœ‰åˆ›å»ºè€…æˆ–ç®¡ç†å‘˜å¯ä»¥å¯¼å…¥è¡¨
        if data_source.created_by != request.user and request.user.role != 'admin':
            return Response({'error': 'æ²¡æœ‰æƒé™è®¿é—®æ­¤æ•°æ®æº'}, status=status.HTTP_403_FORBIDDEN)

        try:
            success, message, dataset_id = DatabaseConnector.import_table_data(
                data_source.connection_config,
                table_name,
                dataset_name,
                request.user,
                data_source
            )

            print(f"å¯¼å…¥ç»“æœ: success={success}, message={message}, dataset_id={dataset_id}")

            if success:
                # åˆ›å»ºæ•°æ®é›†æ´»åŠ¨è®°å½•
                create_dataset_activity(
                    user=request.user,
                    dataset_name=dataset_name,
                    dataset_id=dataset_id,
                    action='created'
                )

                return Response({
                    'success': True,
                    'message': message,
                    'dataset_id': dataset_id
                })
            else:
                return Response({'error': message}, status=status.HTTP_400_BAD_REQUEST)

        except Exception as e:
            print(f"å¯¼å…¥è¡¨æ•°æ®é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return Response({'error': f'å¯¼å…¥è¡¨æ•°æ®æ—¶å‡ºé”™: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    @action(detail=True, methods=['get'])
    def files(self, request, pk=None):
        """è·å–æ•°æ®æºå…³è”çš„æ–‡ä»¶åˆ—è¡¨"""
        data_source = self.get_object()

        # æ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½å¯ä»¥æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨ï¼Œä¸éœ€è¦æƒé™æ£€æŸ¥
        try:
            # è·å–è¯¥æ•°æ®æºä¸‹çš„æ‰€æœ‰æ•°æ®é›†
            datasets = Dataset.objects.filter(data_source=data_source)

            file_list = []
            for dataset in datasets:
                records_count = DataRecord.objects.filter(dataset=dataset).count()

                # ä»æè¿°ä¸­æå–åŸå§‹æ–‡ä»¶å¤§å°
                original_size = 0
                if 'åŸå§‹å¤§å°' in dataset.description:
                    import re
                    size_match = re.search(r'åŸå§‹å¤§å°:\s*(\d+)\s*å­—èŠ‚', dataset.description)
                    if size_match:
                        original_size = int(size_match.group(1))

                # ä¼˜å…ˆä½¿ç”¨åŸå§‹å¤§å°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¼°ç®—
                file_size = original_size if original_size > 0 else records_count * 200 + 2000

                file_info = {
                    'id': dataset.id,
                    'filename': f"{dataset.name}.{dataset.data_type}",
                    'dataset_name': dataset.name,
                    'data_type': dataset.data_type,
                    'size': file_size,  # ä½¿ç”¨å®é™…æˆ–ä¼°ç®—çš„å¤§å°
                    'records_count': records_count,
                    'upload_time': dataset.created_at.isoformat(),
                    'description': dataset.description
                }
                file_list.append(file_info)

            return Response({
                'data_source_id': data_source.id,
                'data_source_name': data_source.name,
                'files': file_list,
                'total_files': len(file_list)
            })

        except Exception as e:
            return Response({'error': f'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class DatasetViewSet(viewsets.ModelViewSet):
    queryset = Dataset.objects.all()
    serializer_class = DatasetSerializer
    permission_classes = [permissions.IsAuthenticated, IsAdminOrReadOnly]

    def get_queryset(self):
        # æ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ•°æ®é›†
        return Dataset.objects.all()

    # ç¡®ä¿æŸ¥çœ‹ç”¨æˆ·æœ‰è¯»å–æƒé™
    def get_permissions(self):
        if self.action in ['list', 'retrieve', 'preview', 'data']:
            # å¯¹äºè¯»å–æ“ä½œï¼Œæ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½æœ‰æƒé™
            return [permissions.IsAuthenticated()]
        # å¯¹äºä¿®æ”¹æ“ä½œï¼Œä¿æŒåŸæœ‰æƒé™æ§åˆ¶
        return [permissions.IsAuthenticated(), IsAdminOrAnalyst()]

    def perform_create(self, serializer):
        print("ğŸ”§ [DEBUG] DatasetViewSet.perform_create è¢«è°ƒç”¨")
        try:
            dataset = serializer.save(created_by=self.request.user)
            print(f"ğŸ”§ [DEBUG] æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {dataset.name}, ID: {dataset.id}")

            # åˆ›å»ºæ´»åŠ¨è®°å½• - ä¿®å¤è°ƒç”¨
            from activities.utils import create_dataset_activity
            result = create_dataset_activity(
                user=self.request.user,
                dataset_name=dataset.name,
                dataset_id=dataset.id,
                action='created'
            )

            if result:
                print(f"âœ… [DEBUG] æ•°æ®é›†æ´»åŠ¨è®°å½•åˆ›å»ºæˆåŠŸ: {result.description}")
            else:
                print(f"âŒ [DEBUG] æ•°æ®é›†æ´»åŠ¨è®°å½•åˆ›å»ºå¤±è´¥")

        except Exception as e:
            print(f"âŒ [DEBUG] åˆ›å»ºæ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    def perform_destroy(self, instance):
        print(f"ğŸ”§ [Dataset] æ­£åœ¨åˆ é™¤æ•°æ®é›†: {instance.name}, ID: {instance.id}")

        # åˆ›å»ºåˆ é™¤æ´»åŠ¨è®°å½•
        create_dataset_activity(
            user=self.request.user,
            dataset_name=instance.name,
            dataset_id=instance.id,
            action='deleted'
        )

        print(f"âœ… [Dataset] æ•°æ®é›†åˆ é™¤æ´»åŠ¨è®°å½•å·²åˆ›å»º")
        super().perform_destroy(instance)

    @action(detail=True, methods=['get'])
    def data(self, request, pk=None):
        """è·å–æ•°æ®é›†çš„æ•°æ®è®°å½•"""
        dataset = self.get_object()
        records = DataRecord.objects.filter(dataset=dataset)

        # è¿”å›æ•°æ®è®°å½•
        data = {
            'dataset_id': dataset.id,
            'dataset_name': dataset.name,
            'records': [
                {
                    'id': record.id,
                    'data': record.data,
                    'created_at': record.created_at
                } for record in records
            ],
            'total_count': records.count()
        }

        return Response(data)

    @action(detail=True, methods=['get'])
    def preview(self, request, pk=None):
        """è·å–æ•°æ®é›†é¢„è§ˆæ•°æ®ï¼ˆæ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½å¯ä»¥è®¿é—®ï¼‰"""
        try:
            dataset = self.get_object()
            limit = int(request.query_params.get('limit', 100))

            # è·å–é™åˆ¶æ•°é‡çš„è®°å½•
            records = DataRecord.objects.filter(dataset=dataset)[:limit]

            # å¤„ç†æ•°æ®æ ¼å¼
            all_data = []
            columns_set = set()

            for record in records:
                record_data = record.data
                if isinstance(record_data, dict) and 'RECORDS' in record_data:
                    if isinstance(record_data['RECORDS'], list):
                        all_data.extend(record_data['RECORDS'])
                    else:
                        all_data.append(record_data['RECORDS'])
                else:
                    all_data.append(record_data)

                # æ”¶é›†æ‰€æœ‰åˆ—å
                if all_data:
                    columns_set.update(all_data[-1].keys())

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
            columns = sorted(list(columns_set))

            return Response({
                'dataset_id': dataset.id,
                'dataset_name': dataset.name,
                'columns': columns,
                'data': all_data,
                'total_records': DataRecord.objects.filter(dataset=dataset).count(),
                'preview_count': len(all_data),
                'limit': limit
            })

        except Exception as e:
            print(f"é¢„è§ˆé”™è¯¯: {str(e)}")
            return Response({'error': f'è·å–é¢„è§ˆæ•°æ®å¤±è´¥: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    @action(detail=True, methods=['get'])
    def export(self, request, pk=None):
        """å¯¼å‡ºæ•°æ®é›†æ•°æ®"""
        try:
            dataset = self.get_object()

            # æ£€æŸ¥ç”¨æˆ·æƒé™
            if dataset.created_by != request.user:
                return Response({'error': 'æ²¡æœ‰æƒé™è®¿é—®æ­¤æ•°æ®é›†'}, status=status.HTTP_403_FORBIDDEN)

            # è·å–æŸ¥è¯¢å‚æ•°
            export_format = request.query_params.get('format', 'csv')
            limit = request.query_params.get('limit')
            page = request.query_params.get('page')
            page_size = request.query_params.get('page_size')

            # æ„å»ºæŸ¥è¯¢
            records_query = DataRecord.objects.filter(dataset=dataset)

            # åº”ç”¨åˆ†é¡µ
            if page and page_size:
                paginator = StandardPagination()
                page_size = int(page_size)
                page = int(page)
                start = (page - 1) * page_size
                end = start + page_size
                records = records_query[start:end]
            elif limit:
                records = records_query[:int(limit)]
            else:
                records = records_query[:1000]  # é»˜è®¤æœ€å¤š1000æ¡

            # å¤„ç†æ•°æ®
            all_data = []
            for record in records:
                record_data = record.data
                if isinstance(record_data, dict) and 'RECORDS' in record_data:
                    if isinstance(record_data['RECORDS'], list):
                        all_data.extend(record_data['RECORDS'])
                    else:
                        all_data.append(record_data['RECORDS'])
                else:
                    all_data.append(record_data)

            # æ ¹æ®æ ¼å¼è¿”å›æ•°æ®
            if export_format == 'csv':
                return self._export_to_csv(all_data, dataset.name)
            else:
                return Response({
                    'data': all_data,
                    'total': len(all_data),
                    'dataset_name': dataset.name
                })

        except Exception as e:
            return Response({'error': f'å¯¼å‡ºæ•°æ®å¤±è´¥: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _export_to_csv(self, data, dataset_name):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        import csv
        from django.http import HttpResponse

        if not data:
            return HttpResponse('No data to export', status=400)

        response = HttpResponse(content_type='text/csv')
        response['Content-Disposition'] = f'attachment; filename="{dataset_name}.csv"'

        # è·å–æ‰€æœ‰åˆ—å
        columns = set()
        for item in data:
            columns.update(item.keys())
        columns = sorted(list(columns))

        writer = csv.DictWriter(response, fieldnames=columns)
        writer.writeheader()

        for item in data:
            # ç¡®ä¿æ¯è¡Œéƒ½æœ‰æ‰€æœ‰åˆ—
            row = {col: item.get(col, '') for col in columns}
            writer.writerow(row)

        return response


class DataRecordViewSet(viewsets.ModelViewSet):
    queryset = DataRecord.objects.all()
    serializer_class = DataRecordSerializer
    permission_classes = [permissions.IsAuthenticated, IsAdminOrReadOnly]

    def get_queryset(self):
        # æ‰€æœ‰è®¤è¯ç”¨æˆ·éƒ½å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ•°æ®è®°å½•
        return DataRecord.objects.all()

    @action(detail=False, methods=['post'])
    def bulk_create(self, request):
        """æ‰¹é‡åˆ›å»ºæ•°æ®è®°å½•"""
        records_data = request.data.get('records', [])
        dataset_id = request.data.get('dataset')

        if not dataset_id or not records_data:
            return Response({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}, status=status.HTTP_400_BAD_REQUEST)

        try:
            dataset = Dataset.objects.get(id=dataset_id, created_by=request.user)
            records_to_create = [
                DataRecord(dataset=dataset, data=record_data)
                for record_data in records_data
            ]

            # ä½¿ç”¨æ‰¹é‡åˆ›å»ºï¼Œè®¾ç½®åˆé€‚çš„æ‰¹æ¬¡å¤§å°
            batch_size = 100
            created_count = 0
            for i in range(0, len(records_to_create), batch_size):
                batch = records_to_create[i:i + batch_size]
                DataRecord.objects.bulk_create(batch)
                created_count += len(batch)

            return Response({
                'message': f'æˆåŠŸåˆ›å»º {created_count} æ¡è®°å½•',
                'created_count': created_count
            })

        except Dataset.DoesNotExist:
            return Response({'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'åˆ›å»ºè®°å½•å¤±è´¥: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class StandardPagination(PageNumberPagination):
    page_size = 20
    page_size_query_param = 'page_size'
    max_page_size = 100
